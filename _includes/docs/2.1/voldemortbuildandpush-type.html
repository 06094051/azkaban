<h2 id='voldemortbuildandpush-type'>VoldemortBuildandPush Type</h2>

<p>Pushing data from hadoop to voldemort store used to be entirely in java. This created lots of problems, mostly due to users having to keep track of jars and dependencies and keep them up-to-date. We created the &#8220;VoldemortBuildandPush&#8221; job type to address this problem. Jars and dependencies are now managed by admins; absolutely no jars or java code are required from users.</p>

<h2 id='howtouse'>How-To-Use</h2>

<p>This is essentially a hadoopJava job, with all jars controlled by the admins. User only need to provide a .job file for the job and specify all the parameters. The following needs to be specified:</p>
<table class='params'><thead><tr><th class='parameter'>Parameter</th><th class='description'> Description</th></tr></thead><tbody><tr><td style='text-align: left;'>type</td><td style='text-align: left;'>The type name as set by the admin, e.g. &#8220;VoldemortBuildandPush&#8221;</td>
</tr><tr><td style='text-align: left;'>push.store.name</td><td style='text-align: left;'>The voldemort push store name</td>
</tr><tr><td style='text-align: left;'>push.store.owners</td><td style='text-align: left;'>The push store owners</td>
</tr><tr><td style='text-align: left;'>push.store.description</td><td style='text-align: left;'>Push store description</td>
</tr><tr><td style='text-align: left;'>build.input.path</td><td style='text-align: left;'>Build input path on hdfs</td>
</tr><tr><td style='text-align: left;'>build.output.dir</td><td style='text-align: left;'>Build output path on hdfs</td>
</tr><tr><td style='text-align: left;'>build.replication.factor</td><td style='text-align: left;'>replication factor number</td>
</tr><tr><td style='text-align: left;'>user.to.proxy</td><td style='text-align: left;'>The hadoop user this job should run under.</td>
</tr><tr><td style='text-align: left;'>build.type.avro</td><td style='text-align: left;'>if build and push avro data, true, otherwise, false</td>
</tr><tr><td style='text-align: left;'>avro.key.field</td><td style='text-align: left;'>if using avro data, key field</td>
</tr><tr><td style='text-align: left;'>avro.value.field</td><td style='text-align: left;'>if using avro data, value field</td>
</tr></tbody></table>
<p>Here are what&#8217;s needed and normally configured by the admn (always put common properties in commonprivate.properties and common.properties for all job types)</p>

<p>These go into private.properties</p>
<table class='params'><thead><tr><th class='parameter'>Parameter</th><th class='description'> Description</th></tr></thead><tbody><tr><td style='text-align: left;'>hadoop.security.manager.class</td><td style='text-align: left;'>The class that handles talking to hadoop clusters.</td>
</tr><tr><td style='text-align: left;'>azkaban.should.proxy</td><td style='text-align: left;'>Whether Azkaban should proxy as individual user hadoop accounts.</td>
</tr><tr><td style='text-align: left;'>proxy.user</td><td style='text-align: left;'>The Azkaban user configured with kerberos and hadoop, for secure clusters.</td>
</tr><tr><td style='text-align: left;'>proxy.keytab.location</td><td style='text-align: left;'>The location of the keytab file with which Azkaban can authenticate with Kerberos for the specified proxy.user</td>
</tr><tr><td style='text-align: left;'>hadoop.home</td><td style='text-align: left;'>The hadoop home where the jars and conf resources are installed.</td>
</tr><tr><td style='text-align: left;'>jobtype.classpath</td><td style='text-align: left;'>The items that every such job should have on its classpath.</td>
</tr><tr><td style='text-align: left;'>jobtype.class</td><td style='text-align: left;'>Should be set to <em>azkaban.jobtype.HadoopJavaJob</em></td>
</tr><tr><td style='text-align: left;'>obtain.binary.token</td><td style='text-align: left;'>Whether Azkaban should request tokens. Set this to true for secure clusters.</td>
</tr><tr><td style='text-align: left;'>azkaban.no.user.classpath</td><td style='text-align: left;'>Set to true such that Azkaban doesn&#8217;t pick up user supplied jars.</td>
</tr></tbody></table>
<p>These go into plugin.properties</p>
<table class='params'><thead><tr><th class='parameter'>Parameter</th><th class='description'> Description</th></tr></thead><tbody><tr><td style='text-align: left;'>job.class</td><td style='text-align: left;'>voldemort.store.readonly.mr.azkaban.VoldemortBuildAndPushJob</td>
</tr><tr><td style='text-align: left;'>voldemort.fetcher.protocol</td><td style='text-align: left;'>webhdfs</td>
</tr><tr><td style='text-align: left;'>hdfs.default.classpath.dir</td><td style='text-align: left;'>HDFS location for distributed cache</td>
</tr><tr><td style='text-align: left;'>hdfs.default.classpath.dir.enable</td><td style='text-align: left;'>set to true if using distributed cache to ship dependency jars</td>
</tr></tbody></table>
<p>Please refer to voldemort project site for more info: <a href='http://www.project-voldemort.com/voldemort/'>project voldemort</a></p>
